{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Baisc libraries\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "#Basic ML libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#MIDI file libraries\n",
    "from mido import MidiFile, Message, MetaMessage, MidiTrack\n",
    "\n",
    "#Keras libraries\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.core import *\n",
    "from keras.layers.normalization import *\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_PER_TIME_SLICE = 0.02\n",
    "HIGHEST_NOTE = 81 \n",
    "LOWEST_NOTE = 33\n",
    "INPUT_DIM = HIGHEST_NOTE - LOWEST_NOTE + 1\n",
    "OUTPUT_DIM = HIGHEST_NOTE - LOWEST_NOTE + 1\n",
    "MICROSECONDS_PER_MINUTE = 60000000\n",
    "\n",
    "NUM_UNITS = 64\n",
    "X_SEQ_LENGTH = 50\n",
    "Y_SEQ_LENGTH = 50\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "OPTIMIZER = Adam()\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DIR = '/home/suraj/FML/scale_chords_small/midi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midiToPianoroll(filepath):\n",
    "    midi_data = MidiFile(filepath)\n",
    "    resolution = midi_data.ticks_per_beat\n",
    "    \n",
    "    set_tempo_events = [x for t in midi_data.tracks for x in t if str(x.type) == 'set_tempo']\n",
    "    tempo = MICROSECONDS_PER_MINUTE/set_tempo_events[0].tempo\n",
    "    \n",
    "    ticks_per_time_slice = 1.0 * (resolution * tempo * TIME_PER_TIME_SLICE)/60 \n",
    "\n",
    "    #find maximum ticks across all tracks\n",
    "    total_ticks =0\n",
    "    for t in midi_data.tracks:\n",
    "        #since ticks represent delta times we need a cumulative sum to get the total ticks in that track\n",
    "        sum_ticks = 0\n",
    "        for e in t:\n",
    "            if str(e.type) == 'note_on' or str(e.type) == 'note_off' or str(e.type) == 'end_of_track':\n",
    "                sum_ticks += e.time\n",
    "        if sum_ticks > total_ticks:\n",
    "            total_ticks = sum_ticks\n",
    "\n",
    "    time_slices = int(ceil(total_ticks / ticks_per_time_slice))\n",
    "    piano_roll = np.zeros((INPUT_DIM, time_slices), dtype =int)\n",
    "\n",
    "    note_states = {}\n",
    "    for track in midi_data.tracks:\n",
    "        total_ticks = 0\n",
    "        for event in track:\n",
    "            if str(event.type) == 'note_on' and event.velocity > 0:\n",
    "                total_ticks += event.time\n",
    "                time_slice_idx = int(total_ticks / ticks_per_time_slice )\n",
    "\n",
    "                if event.note <= HIGHEST_NOTE and event.note >= LOWEST_NOTE: \n",
    "                    note_idx = event.note - LOWEST_NOTE\n",
    "                    piano_roll[note_idx][time_slice_idx] = 1\n",
    "                    note_states[note_idx] = time_slice_idx\n",
    "\n",
    "            elif str(event.type) == 'note_off' or ( str(event.type) == 'note_on' and event.velocity == 0 ):\n",
    "                note_idx = event.note - LOWEST_NOTE\n",
    "                total_ticks += event.time\n",
    "                time_slice_idx = int(total_ticks /ticks_per_time_slice )\n",
    "\n",
    "                if note_idx in note_states:\n",
    "                    last_time_slice_index = note_states[note_idx]\n",
    "                    piano_roll[note_idx][last_time_slice_index:time_slice_idx] = 1\n",
    "                    del note_states[note_idx]\n",
    "    return piano_roll.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir):\n",
    "    pianoroll_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        filepath = data_dir + \"/\" + file\n",
    "        piano_roll = midiToPianoroll(filepath)\n",
    "        pianoroll_data.append(piano_roll)\n",
    "\n",
    "    return pianoroll_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSeqNetInputs(pianoroll_data, x_seq_length, y_seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i,piano_roll in enumerate(pianoroll_data):\n",
    "        pos = 0\n",
    "        while pos + x_seq_length + y_seq_length < piano_roll.shape[0]:\n",
    "            x.append(piano_roll[pos:pos + x_seq_length])\n",
    "            y.append(piano_roll [pos+ x_seq_length: pos + x_seq_length + y_seq_length])\n",
    "            pos += x_seq_length\n",
    "\n",
    "    X = np.array(x)\n",
    "    Y = np.array(y)\n",
    "\n",
    "    x_1, y_1 = shuffle(X,Y)\n",
    "\n",
    "    return x_1, y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSeqTestNetInputs(pianoroll_data, seq_length):\n",
    "    x_test = []\n",
    "\n",
    "    for i,piano_roll in enumerate(pianoroll_data):\n",
    "        x = []\n",
    "        pos = 0\n",
    "        while pos + seq_length < piano_roll.shape[0]:\n",
    "            x.append(piano_roll[pos:pos + seq_length])\n",
    "            pos +=10\n",
    "        x_test.append(np.array(x))\n",
    "\n",
    "    return np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqNetOutToPianoroll(output, threshold = 0.1):\n",
    "    piano_roll = []\n",
    "    for seq_out in output:\n",
    "        for time_slice in seq_out:\n",
    "            idx = [i for i,t in enumerate(time_slice) if t > threshold]\n",
    "            pianoroll_slice = np.zeros(time_slice.shape)\n",
    "            pianoroll_slice[idx] = 1\n",
    "            piano_roll.append(pianoroll_slice)\n",
    "\n",
    "    return np.array(piano_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pianorollToMidi(piano_roll, filepath): \n",
    "    ticks_per_time_slice=1\n",
    "    tempo = 1/TIME_PER_TIME_SLICE\n",
    "    resolution = 60*ticks_per_time_slice/(tempo*TIME_PER_TIME_SLICE)\n",
    "\n",
    "    mid = MidiFile(ticks_per_beat = int(resolution))\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    track.append(MetaMessage('set_tempo', tempo = int(MICROSECONDS_PER_MINUTE/tempo), time =0))\n",
    "\n",
    "    current_state = np.zeros(INPUT_DIM)\n",
    "\n",
    "    index_of_last_event = 0\n",
    "\n",
    "    for slice_index, time_slice in enumerate(np.concatenate((piano_roll, np.zeros((1, INPUT_DIM))), axis =0)):\n",
    "        note_changes = time_slice - current_state\n",
    "\n",
    "        for note_idx, note in enumerate(note_changes):\n",
    "            if note == 1:\n",
    "                note_event = Message('note_on', time = (slice_index - index_of_last_event)*ticks_per_time_slice, velocity = 65, note = note_idx + LOWEST_NOTE )\n",
    "                track.append(note_event)\n",
    "                index_of_last_event = slice_index\n",
    "            elif note == -1:\n",
    "                note_event = Message('note_off', time = (slice_index - index_of_last_event)*ticks_per_time_slice, velocity = 65, note = note_idx + LOWEST_NOTE )\n",
    "                track.append(note_event)\n",
    "                index_of_last_event = slice_index\n",
    "\n",
    "        current_state = time_slice\n",
    "\n",
    "    eot = MetaMessage('end_of_track', time=1)\n",
    "    track.append(eot)\n",
    "\n",
    "    mid.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSeq2Seq_LSTM():\n",
    "    #encoder\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_dim = INPUT_DIM, output_dim = NUM_UNITS, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(NUM_UNITS))\n",
    "\n",
    "    #decoder\n",
    "    model.add(RepeatVector(Y_SEQ_LENGTH))\n",
    "    num_layers= 2\n",
    "    for _ in range(num_layers):\n",
    "        model.add(LSTM(NUM_UNITS, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(TimeDistributed(Dense(OUTPUT_DIM, activation= 'softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSeq2Seq_RNN():\n",
    "    #encoder\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(input_dim = INPUT_DIM, output_dim = NUM_UNITS, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(NUM_UNITS))\n",
    "\n",
    "    #decoder\n",
    "    model.add(RepeatVector(Y_SEQ_LENGTH))\n",
    "    num_layers= 2\n",
    "    for _ in range(num_layers):\n",
    "        model.add(SimpleRNN(NUM_UNITS, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(TimeDistributed(Dense(OUTPUT_DIM, activation= 'softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare training data\n",
    "pianoroll_data = get_data(DIR)\n",
    "input_data, target_data = createSeqNetInputs(pianoroll_data, X_SEQ_LENGTH, Y_SEQ_LENGTH)\n",
    "input_data = input_data.astype(np.bool)\n",
    "target_data = target_data.astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(dropout=0.2, recurrent_dropout=0.2, return_sequences=True, input_shape=(None, 49), units=64)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 64)          7296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 50, 64)            8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 64)            256       \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 50, 64)            8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 64)            256       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 49)            3185      \n",
      "=================================================================\n",
      "Total params: 36,017\n",
      "Trainable params: 35,633\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "7644/7644 [==============================] - 28s 4ms/step - loss: 10.4419\n",
      "Epoch 2/100\n",
      "7644/7644 [==============================] - 18s 2ms/step - loss: 8.8342\n",
      "Epoch 3/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 8.0859\n",
      "Epoch 4/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.8232\n",
      "Epoch 5/100\n",
      "7644/7644 [==============================] - 18s 2ms/step - loss: 7.6962\n",
      "Epoch 6/100\n",
      "7644/7644 [==============================] - 28s 4ms/step - loss: 7.5961\n",
      "Epoch 7/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.5301\n",
      "Epoch 8/100\n",
      "7644/7644 [==============================] - 18s 2ms/step - loss: 7.4881\n",
      "Epoch 9/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.4257\n",
      "Epoch 10/100\n",
      "7644/7644 [==============================] - 19s 2ms/step - loss: 7.3983\n",
      "Epoch 11/100\n",
      "7644/7644 [==============================] - 23s 3ms/step - loss: 7.3874\n",
      "Epoch 12/100\n",
      "7644/7644 [==============================] - 26s 3ms/step - loss: 7.3506\n",
      "Epoch 13/100\n",
      "7644/7644 [==============================] - 23s 3ms/step - loss: 7.3367\n",
      "Epoch 14/100\n",
      "7644/7644 [==============================] - 18s 2ms/step - loss: 7.2996\n",
      "Epoch 15/100\n",
      "7644/7644 [==============================] - 24s 3ms/step - loss: 7.2991\n",
      "Epoch 16/100\n",
      "7644/7644 [==============================] - 23s 3ms/step - loss: 7.2806\n",
      "Epoch 17/100\n",
      "7644/7644 [==============================] - 21s 3ms/step - loss: 7.2792\n",
      "Epoch 18/100\n",
      "7644/7644 [==============================] - 22s 3ms/step - loss: 7.2409\n",
      "Epoch 19/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.2387\n",
      "Epoch 20/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.2394\n",
      "Epoch 21/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.2376\n",
      "Epoch 22/100\n",
      "7644/7644 [==============================] - 21s 3ms/step - loss: 7.2208\n",
      "Epoch 23/100\n",
      "7644/7644 [==============================] - 24s 3ms/step - loss: 7.1993\n",
      "Epoch 24/100\n",
      "7644/7644 [==============================] - 21s 3ms/step - loss: 7.1961\n",
      "Epoch 25/100\n",
      "7644/7644 [==============================] - 22s 3ms/step - loss: 7.1834\n",
      "Epoch 26/100\n",
      "7644/7644 [==============================] - 22s 3ms/step - loss: 7.1856\n",
      "Epoch 27/100\n",
      "7644/7644 [==============================] - 18s 2ms/step - loss: 7.1774\n",
      "Epoch 28/100\n",
      "7644/7644 [==============================] - 19s 2ms/step - loss: 7.1617\n",
      "Epoch 29/100\n",
      "7644/7644 [==============================] - 19s 3ms/step - loss: 7.1495\n",
      "Epoch 30/100\n",
      "7644/7644 [==============================] - 19s 3ms/step - loss: 7.1682\n",
      "Epoch 31/100\n",
      "7644/7644 [==============================] - 19s 3ms/step - loss: 7.1567\n",
      "Epoch 32/100\n",
      "7644/7644 [==============================] - 21s 3ms/step - loss: 7.1686\n",
      "Epoch 33/100\n",
      "7644/7644 [==============================] - 33s 4ms/step - loss: 7.1322\n",
      "Epoch 34/100\n",
      "7644/7644 [==============================] - 26s 3ms/step - loss: 7.1309\n",
      "Epoch 35/100\n",
      "7644/7644 [==============================] - 23s 3ms/step - loss: 7.1368\n",
      "Epoch 36/100\n",
      "7644/7644 [==============================] - 22s 3ms/step - loss: 7.1318\n",
      "Epoch 37/100\n",
      "7644/7644 [==============================] - 22s 3ms/step - loss: 7.1141\n",
      "Epoch 38/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.1337\n",
      "Epoch 39/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.1360\n",
      "Epoch 40/100\n",
      "7644/7644 [==============================] - 22s 3ms/step - loss: 7.1052\n",
      "Epoch 41/100\n",
      "7644/7644 [==============================] - 21s 3ms/step - loss: 7.1085\n",
      "Epoch 42/100\n",
      "7644/7644 [==============================] - 20s 3ms/step - loss: 7.0974\n",
      "Epoch 43/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0912\n",
      "Epoch 44/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.1014\n",
      "Epoch 45/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0984\n",
      "Epoch 46/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0919\n",
      "Epoch 47/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0777\n",
      "Epoch 48/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0814\n",
      "Epoch 49/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0854\n",
      "Epoch 50/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0719\n",
      "Epoch 51/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0808\n",
      "Epoch 52/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0797\n",
      "Epoch 53/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0684\n",
      "Epoch 54/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0553\n",
      "Epoch 55/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0721\n",
      "Epoch 56/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0709\n",
      "Epoch 57/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0644\n",
      "Epoch 58/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0477\n",
      "Epoch 59/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0745\n",
      "Epoch 60/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0459\n",
      "Epoch 61/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0479\n",
      "Epoch 62/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0397\n",
      "Epoch 63/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0340\n",
      "Epoch 64/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0524\n",
      "Epoch 65/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0522\n",
      "Epoch 66/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0233\n",
      "Epoch 67/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0295\n",
      "Epoch 68/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0405\n",
      "Epoch 69/100\n",
      "7644/7644 [==============================] - 18s 2ms/step - loss: 7.0242\n",
      "Epoch 70/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0274\n",
      "Epoch 71/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0179\n",
      "Epoch 72/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0295\n",
      "Epoch 73/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0523\n",
      "Epoch 74/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0272\n",
      "Epoch 75/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0365\n",
      "Epoch 76/100\n",
      "7644/7644 [==============================] - 17s 2ms/step - loss: 7.0489\n"
     ]
    }
   ],
   "source": [
    "#Training RNN\n",
    "model_rnn = createSeq2Seq_RNN()\n",
    "model_rnn.summary()\n",
    "model_rnn.compile(loss=LOSS_FUNCTION, optimizer = OPTIMIZER)\n",
    "earlystop = EarlyStopping(monitor='loss', patience= 10, min_delta = 0.01 , verbose=0, mode= 'auto') \n",
    "history = History()\n",
    "hist = model_rnn.fit(input_data, target_data, batch_size =  BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[ earlystop, history ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(dropout=0.2, recurrent_dropout=0.2, return_sequences=True, input_shape=(None, 49), units=64)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 64)          29184     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 64)            33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50, 64)            33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 64)            256       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 49)            3185      \n",
      "=================================================================\n",
      "Total params: 132,209\n",
      "Trainable params: 131,825\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "7644/7644 [==============================] - 62s 8ms/step - loss: 9.5622\n",
      "Epoch 2/100\n",
      "7644/7644 [==============================] - 59s 8ms/step - loss: 8.0487\n",
      "Epoch 3/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 7.5144\n",
      "Epoch 4/100\n",
      "7644/7644 [==============================] - 63s 8ms/step - loss: 7.2927\n",
      "Epoch 5/100\n",
      "7644/7644 [==============================] - 58s 8ms/step - loss: 7.1641\n",
      "Epoch 6/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 7.0617\n",
      "Epoch 7/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.9861\n",
      "Epoch 8/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.8967\n",
      "Epoch 9/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.8400\n",
      "Epoch 10/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 6.7762\n",
      "Epoch 11/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.7208\n",
      "Epoch 12/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.6877\n",
      "Epoch 13/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.6555\n",
      "Epoch 14/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.6218\n",
      "Epoch 15/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.6070\n",
      "Epoch 16/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.5707\n",
      "Epoch 17/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.5609\n",
      "Epoch 18/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.5182\n",
      "Epoch 19/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.4920\n",
      "Epoch 20/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.4597\n",
      "Epoch 21/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.4385\n",
      "Epoch 22/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.4309\n",
      "Epoch 23/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.3956\n",
      "Epoch 24/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.3787\n",
      "Epoch 25/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.3677\n",
      "Epoch 26/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.3508\n",
      "Epoch 27/100\n",
      "7644/7644 [==============================] - 570s 75ms/step - loss: 6.3292\n",
      "Epoch 28/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.2915\n",
      "Epoch 29/100\n",
      "7644/7644 [==============================] - 59s 8ms/step - loss: 6.2909\n",
      "Epoch 30/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.2775\n",
      "Epoch 31/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.2583\n",
      "Epoch 32/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 6.2283\n",
      "Epoch 33/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.2493\n",
      "Epoch 34/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.2145\n",
      "Epoch 35/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.1992\n",
      "Epoch 36/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.1863\n",
      "Epoch 37/100\n",
      "7644/7644 [==============================] - 57s 7ms/step - loss: 6.1574\n",
      "Epoch 38/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.1434\n",
      "Epoch 39/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.1345\n",
      "Epoch 40/100\n",
      "7644/7644 [==============================] - 57s 7ms/step - loss: 6.1255\n",
      "Epoch 41/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.1167\n",
      "Epoch 42/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0953\n",
      "Epoch 43/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0882\n",
      "Epoch 44/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0884\n",
      "Epoch 45/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0820\n",
      "Epoch 46/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0600\n",
      "Epoch 47/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0405\n",
      "Epoch 48/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0294\n",
      "Epoch 49/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0068\n",
      "Epoch 50/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0340\n",
      "Epoch 51/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 6.0118\n",
      "Epoch 52/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0319\n",
      "Epoch 53/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0038\n",
      "Epoch 54/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 6.0021\n",
      "Epoch 55/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9668\n",
      "Epoch 56/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9518\n",
      "Epoch 57/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9546\n",
      "Epoch 58/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9384\n",
      "Epoch 59/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9229\n",
      "Epoch 60/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9352\n",
      "Epoch 61/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9187\n",
      "Epoch 62/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 5.9163\n",
      "Epoch 63/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9046\n",
      "Epoch 64/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.9054\n",
      "Epoch 65/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 5.8925\n",
      "Epoch 66/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8939\n",
      "Epoch 67/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8767\n",
      "Epoch 68/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8739\n",
      "Epoch 69/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8608\n",
      "Epoch 70/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 5.8659\n",
      "Epoch 71/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8775\n",
      "Epoch 72/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8390\n",
      "Epoch 73/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 5.8333\n",
      "Epoch 74/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8676\n",
      "Epoch 75/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8321\n",
      "Epoch 76/100\n",
      "7644/7644 [==============================] - 55s 7ms/step - loss: 5.8272\n",
      "Epoch 77/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8181\n",
      "Epoch 78/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.8163\n",
      "Epoch 79/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.8074\n",
      "Epoch 80/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.8075\n",
      "Epoch 81/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7939\n",
      "Epoch 82/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7929\n",
      "Epoch 83/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.7951\n",
      "Epoch 84/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.7673\n",
      "Epoch 85/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7784\n",
      "Epoch 86/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7781\n",
      "Epoch 87/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7603\n",
      "Epoch 88/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7627\n",
      "Epoch 89/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.7636\n",
      "Epoch 90/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7465\n",
      "Epoch 91/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7764\n",
      "Epoch 92/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7456\n",
      "Epoch 93/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7331\n",
      "Epoch 94/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7325\n",
      "Epoch 95/100\n",
      "7644/7644 [==============================] - 54s 7ms/step - loss: 5.7129\n",
      "Epoch 96/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7118\n",
      "Epoch 97/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7019\n",
      "Epoch 98/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.6989\n",
      "Epoch 99/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.7021\n",
      "Epoch 100/100\n",
      "7644/7644 [==============================] - 53s 7ms/step - loss: 5.6924\n"
     ]
    }
   ],
   "source": [
    "#Training LSTM\n",
    "model_lstm = createSeq2Seq_LSTM()\n",
    "model_lstm.summary()\n",
    "model_lstm.compile(loss=LOSS_FUNCTION, optimizer = OPTIMIZER)\n",
    "earlystop = EarlyStopping(monitor='loss', patience= 10, min_delta = 0.01 , verbose=0, mode= 'auto') \n",
    "history = History()\n",
    "hist = model_lstm.fit(input_data, target_data, batch_size =  BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[ earlystop, history ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_piano_roll = midiToPianoroll('/home/suraj/FML/scale_chords_small/midi/scale_b_phrygian.mid')\n",
    "test_data = [test_piano_roll]\n",
    "test_input = createSeqTestNetInputs(test_data, X_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate music using RNN\n",
    "for i,song in enumerate(test_input):\n",
    "    net_output = model_rnn.predict(song)\n",
    "    net_roll = seqNetOutToPianoroll(net_output)\n",
    "    pianorollToMidi(net_roll, 'output_harmony_rnn.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate music using LSTM\n",
    "for i,song in enumerate(test_input):\n",
    "    net_output = model_lstm.predict(song)\n",
    "    net_roll = seqNetOutToPianoroll(net_output)\n",
    "    pianorollToMidi(net_roll, 'output_harmony_lstm.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
